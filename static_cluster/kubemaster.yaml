#cloud-config

hostname: "kubemaster"

ssh_authorized_keys:
  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCWM7xMnzcy1hTjxoV7/Cc8lSeoWhUUPsoNbFSkL9wiLq++SqscipEeA3HRYlI7X9VJbhCWRYAA5AKj7hGMZ60sByzvo8nSJVN54zcOfH5wnJKdO3FJeQfYkyqYsBpC8Kg0+dILtvhxgf5KenFQkPnb8DXmNExvm/jzJgu2YbOSmKcSvt9zQnHTj7AAILC8GozYonJEzE5Yo8JiKUbVSBq9lL54dbi/47QhfhIjKbK7n4+tbsU/aLPKauv+NW2jj69tIycg6WnoarSeUNZy8/zUtcMqwfrilxuDw+9Gl+o+aXeVbwpkOw6D0bPeWkOwY0CkdyOKfwiW5tP4VUF70VOr drigocar@AC049707
  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDIjWNEUigHQV8Hjnb64XhrG6xApRNPhgxh1pUj3zrS0v5CsHT46ami3fceoNhsWsbl/A89xvMjBAg2mW74QzuK1sKfJQBLulpEPbILi0g2xOKL6M0LJSDmIfQPMkJQ6tWzb33dUZw2efLZsamseiKOTlj5zP2Kz1zSEVzL9mgt16T8+werKw5sRMvJjyBZY0QXCbAG7mkmjJxgGRPgBgV11fGCNyUV+TJ7h2u0HKpEC5A+PeTw2ZNHzKZc8nGtpkMbnUIT5WjjWxbOj2kDuyTGDwt8ix1ZVT3PFE0JQ9C3Jz4TFPKdl+Sfj5rYQ7W+lSIVkBo1zJtiZzJTdssdfxlv drigocar@AC049707

users:
  - name: "sysadmin"
    passwd: "$1$GFuzo/U6$aF7g4o.S2OE9iv9v/On61."
    groups:
      - "sudo"
      - "docker"
    ssh-authorized-keys:
    - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCWM7xMnzcy1hTjxoV7/Cc8lSeoWhUUPsoNbFSkL9wiLq++SqscipEeA3HRYlI7X9VJbhCWRYAA5AKj7hGMZ60sByzvo8nSJVN54zcOfH5wnJKdO3FJeQfYkyqYsBpC8Kg0+dILtvhxgf5KenFQkPnb8DXmNExvm/jzJgu2YbOSmKcSvt9zQnHTj7AAILC8GozYonJEzE5Yo8JiKUbVSBq9lL54dbi/47QhfhIjKbK7n4+tbsU/aLPKauv+NW2jj69tIycg6WnoarSeUNZy8/zUtcMqwfrilxuDw+9Gl+o+aXeVbwpkOw6D0bPeWkOwY0CkdyOKfwiW5tP4VUF70VOr drigocar@AC049707

write_files:

    # Set up hosts file
    - path: /etc/hosts
      owner: root
      content: |
             192.168.1.5    node1 node1.ellesmera.intranet
             10.1.0.5       worker1 worker1.ellesmera.intranet
             192.168.1.6    node2 node2.ellesmera.intranet
             10.1.0.6       worker2 worker2.ellesmera.intranet
             192.168.1.7    node3 node3.ellesmera.intranet
             10.1.0.7       worker3 worker3.ellesmera.intranet
             192.168.1.20   kubemaster kubemaster.ellesmera.intranet
             10.1.0.20      master1 master1.ellesmera.intranet

    # setup resolv.conf file
    - path: /etc/resolv.conf
      content: |
        search ellesmera
        nameserver 192.168.1.1

    # create directories structure
    - path: /opt/bin/create_dirs
      owner: root
      permissions: 0755
      content: |
          #!/usr/bin/bash
          mkdir -p /etc/kubernetes/
          mkdir -p /etc/kubernetes/ssl/
          mkdir -p /etc/kubernetes/manifests/
          mkdir -p /etc/flannel/
          mkdir -p /etc/systemd/system/flanneld.service.d/
          mkdir -p /etc/systemd/system/docker.service.d/
          mkdir -p /etc/kubernetes/cni/net.d/

    # Configure ca-certificates
    - path: /etc/kubernetes/ssl/openssl.cnf
      owner: root
      content: |
          [req]
          req_extensions = v3_req
          distinguished_name = req_distinguished_name
          [req_distinguished_name]
          [ v3_req ]
          basicConstraints = CA:FALSE
          keyUsage = nonRepudiation, digitalSignature, keyEncipherment
          subjectAltName = @alt_names
          [alt_names]
          DNS.1 = kubernetes
          DNS.2 = kubernetes.default
          DNS.3 = kubernetes.default.svc
          DNS.4 = kubernetes.default.svc.cluster.local
          IP.1 = ${K8S_SERVICE_IP}
          IP.2 = ${MASTER_HOST}

    # Kubernetes Worker Keypairs
    - path: /etc/kubernetes/ssl/worker-openssl.cnf
      owner: root
      content: |
          [req]
          req_extensions = v3_req
          distinguished_name = req_distinguished_name
          [req_distinguished_name]
          [ v3_req ]
          basicConstraints = CA:FALSE
          keyUsage = nonRepudiation, digitalSignature, keyEncipherment
          subjectAltName = @alt_names
          [alt_names]
          IP.1 = $ENV::WORKER_IP

    # Kubernetes Script Install
    - path: /opt/bin/kubernetes-install.sh
      owner: root
      permissions: 0755
      content: |
        #! /usr/bin/bash
        if [ ! -f /opt/bin/kubelet ]; then
          echo "Kubenetes not installed - installing."
          # Extract the Kubernetes binaries.
          wget -N -P /opt/bin http://storage.googleapis.com/kubernetes-release/release/v1.6.1/bin/linux/amd64/kubectl
          wget -N -P /opt/bin http://storage.googleapis.com/kubernetes-release/release/v1.6.1/bin/linux/amd64/kubelet
          sudo chmod +x /opt/bin/kubelet /opt/bin/kubectl
        fi

    # Calico Script Install
    - path: /opt/bin/calico-install.sh
      owner: root
      permissions: 0755
      content: |
        #! /usr/bin/bash
        if [ ! -f /opt/bin/calicoctl ]; then
          echo "Calico not installed - installing."
          # Install the `calicoctl` binary
          wget https://github.com/projectcalico/calicoctl/releases/download/v1.0.2/calicoctl
          chmod +x calicoctl
          mv calicoctl /opt/bin
          # Fetch the calico/node container
          docker pull calico/node:v0.14.0
        fi

    #Kubernetes Script Config
    - path: /etc/kubernetes/ssl/kubeconfig.sh
      owner: root
      permissions: 0755
      content: |
        #!/usr/bin/bash

        #set -x
        
        ###################################
        # Script configuracao Kubernetes
        ###################################

        function main()
        {
          echo "Begin setup Cluster ...";
          echo 'core:core' | chpasswd

          # openssl.cnf setup
          MASTER_IP=192.168.1.20
          K8S_IP=10.3.0.1

          # kubernetes workers setup
          WORKER_NUMBER=3
          WORKERS_FQDN=("worker1" "worker2" "worker3")
          WORKERS_IP=("192.168.1.5" "192.168.1.6" "192.168.1.7")
          MASTER_IP=("192.168.1.20")
          K8S_SERVICE_IP=10.3.0.1

          # kubernetes master setup
          ADV_IP=192.168.1.20
          ETCD_END=http://10.1.0.20:2379
          K8S=v1.6.1_coreos.0
          NETWORK=
          SERVICE_RANGE=10.3.0.0/24
          DNS_SERVICE=10.3.0.10

          # kubernetes certificates setup
          CA_CERT=/etc/kubernetes/ssl/ca.pem
          ADMIN_KEY=/etc/kubernetes/ssl/admin-key.pem
          ADMIN_CERT=/etc/kubernetes/ssl/admin.pem
          MASTER_HOST=192.168.1.20

          create_ca

        }
        function create_ca()
        {

          echo "Create a Cluster Root CA ...";

          if [ ! -f /etc/kubernetes/ssl/ca-key.pem ]; then
            openssl genrsa -out ca-key.pem 2048
            openssl req -x509 -new -nodes -key ca-key.pem -days 10000 -out ca.pem -subj "/CN=kube-ca"
          fi

          config_cluster

        }

        function config_cluster()
        {

          echo "Set up IPs ...";

          eval sed -i -e "s#\\\${MASTER_HOST}#"$MASTER_IP"#g" openssl.cnf
          eval sed -i -e "s#\\\${K8S_SERVICE_IP}#"$K8S_IP"#g" openssl.cnf
          echo "Done.";

          create_keypair

        }


        function create_keypair()
        {

          echo "Generate the API Server Keypair ...";

          if [ ! -f apiserver-key.pem -a ! -f apiserver.csr ]; then
           openssl genrsa -out apiserver-key.pem 2048
           openssl req -new -key apiserver-key.pem -out apiserver.csr -subj "/CN=kube-apiserver" -config openssl.cnf
           openssl x509 -req -in apiserver.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out apiserver.pem -days 365 -extensions v3_req -extfile openssl.cnf
          fi
          echo "Done ...";

          create_worker

        }

        function create_worker()
        {

          echo "Generate the Kubernetes Worker Keypairs ...";

          if [ ! -f /etc/kubernetes/ssl/worker*.pem -a ! -f /etc/kubernetes/ssl/worker*.csr ]; then
            echo "Generating TLS keys."
            openssl genrsa -out worker-key.pem 2048
            WORKER_IP="${MASTER_IP}" openssl req -new -key worker-key.pem -out worker.csr -subj "/CN=worker" -config worker-openssl.cnf
            WORKER_IP="${MASTER_IP}" openssl x509 -req -in worker.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf

            for ((i=0; i <= "$WORKER_NUMBER" - 1; i++));
            do
              echo "${WORKERS_IP[i]}" --- "${WORKERS_FQDN[i]}";
              openssl genrsa -out "${WORKERS_FQDN[i]}"-worker-key.pem 2048
              WORKER_IP="${WORKERS_IP[i]}" openssl req -new -key "${WORKERS_FQDN[i]}"-worker-key.pem -out "${WORKERS_FQDN[i]}"-worker.csr -subj "/CN="${WORKERS_FQDN[i]}"" -config worker-openssl.cnf
              WORKER_IP="${WORKERS_IP[i]}" openssl x509 -req -in "${WORKERS_FQDN[i]}"-worker.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out "${WORKERS_FQDN[i]}"-worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf
              sleep 2;
              tar cfv "${WORKERS_FQDN[i]}".tgz ca.pem "${WORKERS_FQDN[i]}"-worker-key.pem "${WORKERS_FQDN[i]}"-worker.pem
              tar tf "${WORKERS_FQDN[i]}".tgz
              scp "${WORKERS_FQDN[i]}".tgz core@"${WORKERS_FQDN[i]}":~/
            done
          fi

          create_adminkey

        }

        function create_adminkey()
        {

          # Generate the Cluster Administrator Keypair

          if [ ! -f admin-key.pem -a ! -f admin.csr ]; then
           openssl genrsa -out admin-key.pem
           openssl req -new -key admin-key.pem -out admin.csr -subj "/CN=kube-admin"
           openssl x509 -req -in admin.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin.pem -days 365
          fi

          create_dirs

        }

        function create_dirs()
        {

          # Automated creatre directories
          sh create_dirs

          kubernetes_setup

        }

        function kubernetes_setup()
        {

          echo "Flannel Network Configuration ..."
          eval sed -i -e 's#\\\${ADVERTISE_IP}#"$ADV_IP"#g' /etc/flannel/options.env
          eval sed -i -e 's#\\\${ETCD_ENDPOINTS}#"$ETCD_END"#g' /etc/flannel/options.env

          echo "Create the kubelet Unit ..."
          eval sed -i -e 's#\\\${ADVERTISE_IP}#"$ADV_IP"#g' /etc/systemd/system/kubelet.service
          eval sed -i -e 's#\\\${K8S_VER}#"$K8S"#g' /etc/systemd/system/kubelet.service
          eval sed -i -e 's#\\\${DNS_SERVICE_IP}#"$DNS_SERVICE"#g' /etc/systemd/system/kubelet.service
          eval sed -i -e 's#\\\${NETWORK_PLUGIN}#"$NETWORK"#g' /etc/systemd/system/kubelet.service

          echo "Set Up the kube-apiserver Pod ..."
          eval sed -i -e 's#\\\${ETCD_ENDPOINTS}#"$ETCD_END"#g' /etc/kubernetes/manifests/kube-apiserver.yaml
          eval sed -i -e 's#\\\${SERVICE_IP_RANGE}#"$SERVICE_RANGE"#g' /etc/kubernetes/manifests/kube-apiserver.yaml
          eval sed -i -e 's#\\\${ADVERTISE_IP}#"$ADV_IP"#g' /etc/kubernetes/manifests/kube-apiserver.yaml
          eval sed -i -e 's#\\\${K8S_VER}#"$K8S"#g' /etc/kubernetes/manifests/kube-apiserver.yaml

          echo "Set Up the kube-controller-manager Pod ..."
          eval sed -i -e 's#\\\${K8S_VER}#"$K8S"#g' /etc/kubernetes/manifests/kube-controller-manager.yaml

          echo "Set Up the kube-scheduler Pod ..."
          eval sed -i -e 's#\\\${K8S_VER}#"$K8S"#g' /etc/kubernetes/manifests/kube-scheduler.yaml

          echo "Set Up the kube-proxy Pod ..."
          eval sed -i -e 's#\\\${K8S_VER}#"$K8S"#g' /etc/kubernetes/manifests/kube-proxy.yaml

          echo "Set Up the CNI config (optional) ..."
          eval sed -i -e 's#\\\${ETCD_ENDPOINTS}#"$ETCD_END"#g' /etc/kubernetes/manifests/calico.yaml

          setup_certs

        }

        function setup_certs()
        {

          echo "TLS Assets ...";
          chmod 600 /etc/kubernetes/ssl/*-key.pem
          chown root:root /etc/kubernetes/ssl/*-key.pem

          daemon_reload

        }

        function daemon_reload()
        {

          echo "Saving settings ..."
          systemctl daemon-reload
          systemctl restart etcd2

        }

        function setupkube()
        {

          sh /opt/bin/kubernetes-install.sh
          sh /opt/bin/calico-install.sh

          /opt/bin/kubectl config set-cluster default-cluster --server=https://"${MASTER_HOST}" --certificate-authority="${CA_CERT}"
          /opt/bin/kubectl config set-credentials default-admin --certificate-authority="${CA_CERT}" --client-key="${ADMIN_KEY}" --client-certificate="${ADMIN_CERT}"
          /opt/bin/kubectl config set-context default-system --cluster=default-cluster --user=default-admin
          /opt/bin/kubectl config use-context default-system

          curl -H "Content-Type: application/json" -XPOST -d'{"apiVersion":"v1","kind":"Namespace","metadata":{"name":"kube-system"}}' "http://127.0.0.1:8080/api/v1/namespaces"
          curl -H "Content-Type: application/json" -XPOST -d'{"apiVersion":"v1","kind":"Namespace","metadata":{"name":"calico-system"}}' "http://127.0.0.1:8080/api/v1/namespaces"

          valid_kubernetes

        }

        function valid_kubernetes()
        {
          curl http://127.0.0.1:8080/version
          curl -s localhost:10255/pods | jq -r '.items[].metadata.name'

        }
        ############### Start Script ####################
        $@

    # Network Configuration flannel
    - path: /etc/flannel/options.env
      owner: root
      content: |
          FLANNELD_IFACE=${ADVERTISE_IP}
          FLANNELD_ETCD_ENDPOINTS=${ETCD_ENDPOINTS}

    # Network Configuration flannel
    - path: /etc/systemd/system/flanneld.service.d/40-ExecStartPre-symlink.conf
      content: |
          [Service]
          ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env

    # Docker Configuration
    - path: /etc/systemd/system/docker.service.d/40-flannel.conf
      owner: root
      content: |
        [Unit]
        Requires=flanneld.service
        After=flanneld.service
        [Service]
        EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env

    # Docker CNI Configuration
    - path: /etc/kubernetes/cni/docker_opts_cni.env
      owner: root
      content: |
        DOCKER_OPT_BIP=""
        DOCKER_OPT_IPMASQ=""

    # Flannel CNI Configuration
    - path: /etc/kubernetes/cni/net.d/10-flannel.conf
      owner: root
      content: |
        {
            "name": "podnet",
            "type": "flannel",
            "delegate": {
                "isDefaultGateway": true
            }
        }

    # Create the kubelet Unit
    - path: /etc/systemd/system/kubelet.service
      content: |
        [Service]
        Environment=KUBELET_IMAGE_TAG=${K8S_VER}
        Environment="RKT_RUN_ARGS=--uuid-file-save=/var/run/kubelet-pod.uuid \
          --volume var-log,kind=host,source=/var/log \
          --mount volume=var-log,target=/var/log \
          --volume dns,kind=host,source=/etc/resolv.conf \
          --mount volume=dns,target=/etc/resolv.conf \
          --volume cni-bin,kind=host,source=/opt/cni/bin \
          --mount volume=cni-bin,target=/opt/cni/bin"
        ExecStartPre=/usr/bin/mkdir -p /opt/cni/bin
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/run/kubelet-pod.uuid
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
          --api-servers=http://127.0.0.1:8080 \
          --register-schedulable=false \
          --cni-conf-dir=/etc/kubernetes/cni/net.d \
          --network-plugin=${NETWORK_PLUGIN} \
          --container-runtime=docker \
          --allow-privileged=true \
          --pod-manifest-path=/etc/kubernetes/manifests \
          --hostname-override=${ADVERTISE_IP} \
          --cluster_dns=${DNS_SERVICE_IP} \
          --cluster_domain=cluster.local
        ExecStop=-/usr/bin/rkt stop --uuid-file=/var/run/kubelet-pod.uuid
        Restart=always
        RestartSec=10

        [Install]
        WantedBy=multi-user.target

    # Set Up the kube-apiserver Pod
    - path: /etc/kubernetes/manifests/kube-apiserver.yaml
      owner: root
      content: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-apiserver
          namespace: kube-system
        spec:
          hostNetwork: true
          containers:
          - name: kube-apiserver
            image: quay.io/coreos/hyperkube:${K8S_VER}
            command:
            - /hyperkube
            - apiserver
            - --bind-address=0.0.0.0
            - --etcd-servers=${ETCD_ENDPOINTS}
            - --storage-backend=etcd2
            - --allow-privileged=true
            - --service-cluster-ip-range=${SERVICE_IP_RANGE}
            - --secure-port=443
            - --advertise-address=${ADVERTISE_IP}
            - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota
            - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
            - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
            - --client-ca-file=/etc/kubernetes/ssl/ca.pem
            - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
            - --runtime-config=extensions/v1beta1/networkpolicies=true
            - --anonymous-auth=false
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                port: 8080
                path: /healthz
              initialDelaySeconds: 15
              timeoutSeconds: 15
            ports:
            - containerPort: 443
              hostPort: 443
              name: https
            - containerPort: 8080
              hostPort: 8080
              name: local
            volumeMounts:
            - mountPath: /etc/kubernetes/ssl
              name: ssl-certs-kubernetes
              readOnly: true
            - mountPath: /etc/ssl/certs
              name: ssl-certs-host
              readOnly: true
          volumes:
          - hostPath:
              path: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
          - hostPath:
              path: /usr/share/ca-certificates
            name: ssl-certs-host

    # Set Up the kube-proxy Pod
    - path: /etc/kubernetes/manifests/kube-proxy.yaml
      owner: root
      content: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-proxy
          namespace: kube-system
        spec:
          hostNetwork: true
          containers:
          - name: kube-proxy
            image: quay.io/coreos/hyperkube:${K8S_VER}
            command:
            - /hyperkube
            - proxy
            - --master=http://127.0.0.1:8080
            securityContext:
              privileged: true
            volumeMounts:
            - mountPath: /etc/ssl/certs
              name: ssl-certs-host
              readOnly: true
          volumes:
          - hostPath:
              path: /usr/share/ca-certificates
            name: ssl-certs-host

    # Set Up the kube-controller-manager Pod
    - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
      owner: root
      content: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-controller-manager
          namespace: kube-system
        spec:
          hostNetwork: true
          containers:
          - name: kube-controller-manager
            image: quay.io/coreos/hyperkube:${K8S_VER}
            command:
            - /hyperkube
            - controller-manager
            - --master=http://127.0.0.1:8080
            - --leader-elect=true
            - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
            - --root-ca-file=/etc/kubernetes/ssl/ca.pem
            resources:
              requests:
                cpu: 200m
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                path: /healthz
                port: 10252
              initialDelaySeconds: 15
              timeoutSeconds: 15
            volumeMounts:
            - mountPath: /etc/kubernetes/ssl
              name: ssl-certs-kubernetes
              readOnly: true
            - mountPath: /etc/ssl/certs
              name: ssl-certs-host
              readOnly: true
          hostNetwork: true
          volumes:
          - hostPath:
              path: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
          - hostPath:
              path: /usr/share/ca-certificates
            name: ssl-certs-host

    # Set Up the kube-scheduler Pod
    - path: /etc/kubernetes/manifests/kube-scheduler.yaml
      owner: root
      content: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-scheduler
          namespace: kube-system
        spec:
          hostNetwork: true
          containers:
          - name: kube-scheduler
            image: quay.io/coreos/hyperkube:${K8S_VER}
            command:
            - /hyperkube
            - scheduler
            - --master=http://127.0.0.1:8080
            - --leader-elect=true
            resources:
              requests:
                cpu: 100m
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                path: /healthz
                port: 10251
              initialDelaySeconds: 15
              timeoutSeconds: 15

    # Set up Calico Network
    - path: /etc/kubernetes/manifests/calico.yaml
      owner: root
      content: |
        # This ConfigMap is used to configure a self-hosted Calico installation.
        kind: ConfigMap
        apiVersion: v1
        metadata:
          name: calico-config
          namespace: kube-system
        data:
          # Configure this with the location of your etcd cluster.
          etcd_endpoints: "${ETCD_ENDPOINTS}"

          # The CNI network configuration to install on each node.  The special
          # values in this config will be automatically populated.
          cni_network_config: |-
            {
                "name": "calico",
                "type": "flannel",
                "delegate": {
                  "type": "calico",
                  "etcd_endpoints": "__ETCD_ENDPOINTS__",
                  "log_level": "info",
                  "policy": {
                      "type": "k8s",
                      "k8s_api_root": "https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__",
                      "k8s_auth_token": "__SERVICEACCOUNT_TOKEN__"
                  },
                  "kubernetes": {
                      "kubeconfig": "/etc/kubernetes/cni/net.d/__KUBECONFIG_FILENAME__"
                  }
                }
            }

        ---

        # This manifest installs the calico/node container, as well
        # as the Calico CNI plugins and network config on
        # each master and worker node in a Kubernetes cluster.
        kind: DaemonSet
        apiVersion: extensions/v1beta1
        metadata:
          name: calico-node
          namespace: kube-system
          labels:
            k8s-app: calico-node
        spec:
          selector:
            matchLabels:
              k8s-app: calico-node
          template:
            metadata:
              labels:
                k8s-app: calico-node
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
                scheduler.alpha.kubernetes.io/tolerations: |
                  [{"key": "dedicated", "value": "master", "effect": "NoSchedule" },
                   {"key":"CriticalAddonsOnly", "operator":"Exists"}]
            spec:
              hostNetwork: true
              containers:
                # Runs calico/node container on each Kubernetes node.  This
                # container programs network policy and routes on each
                # host.
                - name: calico-node
                  image: quay.io/calico/node:v0.23.0
                  env:
                    # The location of the Calico etcd cluster.
                    - name: ETCD_ENDPOINTS
                      valueFrom:
                        configMapKeyRef:
                          name: calico-config
                          key: etcd_endpoints
                    # Choose the backend to use.
                    - name: CALICO_NETWORKING_BACKEND
                      value: "none"
                    # Disable file logging so `kubectl logs` works.
                    - name: CALICO_DISABLE_FILE_LOGGING
                      value: "true"
                    - name: NO_DEFAULT_POOLS
                      value: "true"
                  securityContext:
                    privileged: true
                  volumeMounts:
                    - mountPath: /lib/modules
                      name: lib-modules
                      readOnly: false
                    - mountPath: /var/run/calico
                      name: var-run-calico
                      readOnly: false
                    - mountPath: /etc/resolv.conf
                      name: dns
                      readOnly: true
                # This container installs the Calico CNI binaries
                # and CNI network config file on each node.
                - name: install-cni
                  image: quay.io/calico/cni:v1.5.2
                  imagePullPolicy: Always
                  command: ["/install-cni.sh"]
                  env:
                    # CNI configuration filename
                    - name: CNI_CONF_NAME
                      value: "10-calico.conf"
                    # The location of the Calico etcd cluster.
                    - name: ETCD_ENDPOINTS
                      valueFrom:
                        configMapKeyRef:
                          name: calico-config
                          key: etcd_endpoints
                    # The CNI network config to install on each node.
                    - name: CNI_NETWORK_CONFIG
                      valueFrom:
                        configMapKeyRef:
                          name: calico-config
                          key: cni_network_config
                  volumeMounts:
                    - mountPath: /host/opt/cni/bin
                      name: cni-bin-dir
                    - mountPath: /host/etc/cni/net.d
                      name: cni-net-dir
              volumes:
                # Used by calico/node.
                - name: lib-modules
                  hostPath:
                    path: /lib/modules
                - name: var-run-calico
                  hostPath:
                    path: /var/run/calico
                # Used to install CNI.
                - name: cni-bin-dir
                  hostPath:
                    path: /opt/cni/bin
                - name: cni-net-dir
                  hostPath:
                    path: /etc/kubernetes/cni/net.d
                - name: dns
                  hostPath:
                    path: /etc/resolv.conf

        ---

        # This manifest deploys the Calico policy controller on Kubernetes.
        # See https://github.com/projectcalico/k8s-policy
        apiVersion: extensions/v1beta1
        kind: ReplicaSet
        metadata:
          name: calico-policy-controller
          namespace: kube-system
          labels:
            k8s-app: calico-policy
        spec:
          # The policy controller can only have a single active instance.
          replicas: 1
          template:
            metadata:
              name: calico-policy-controller
              namespace: kube-system
              labels:
                k8s-app: calico-policy
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
                scheduler.alpha.kubernetes.io/tolerations: |
                  [{"key": "dedicated", "value": "master", "effect": "NoSchedule" },
                   {"key":"CriticalAddonsOnly", "operator":"Exists"}]
            spec:
              # The policy controller must run in the host network namespace so that
              # it isn't governed by policy that would prevent it from working.
              hostNetwork: true
              containers:
                - name: calico-policy-controller
                  image: calico/kube-policy-controller:v0.4.0
                  env:
                    # The location of the Calico etcd cluster.
                    - name: ETCD_ENDPOINTS
                      valueFrom:
                        configMapKeyRef:
                          name: calico-config
                          key: etcd_endpoints
                    # The location of the Kubernetes API.  Use the default Kubernetes
                    # service for API access.
                    - name: K8S_API
                      value: "https://kubernetes.default:443"
                    # Since we're running in the host namespace and might not have KubeDNS
                    # access, configure the container's /etc/hosts to resolve
                    # kubernetes.default to the correct service clusterIP.
                    - name: CONFIGURE_ETC_HOSTS
                      value: "true"

coreos:
  update:
      reboot-strategy: off

  units:
    - name: static.network
      content: |
        [Match]
        Name=eth0
        [Network]
        Address=10.1.0.20/24

    - name: 11-static.network
      content: |
        [Match]
        Name=eth1
        [Network]
        Address=10.2.0.20/24

    - name: 13-static.network
      content: |
        [Match]
        Name=eth2
        [Network]
        Address=192.168.1.20/24
        Gateway=192.168.1.1

    - name: cpu-governor.service
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Enable CPU power saving
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStart=/usr/sbin/modprobe cpufreq_powersave

    - name: systemd-networkd.service
      command: start
      enable: true

    - name: kubeconfig.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Execute Kubeconfig Service
        After=systemd-networkd.service
        Requires=systemd-networkd.service

        [Service]
        Type=forking
        WorkingDirectory=/etc/kubernetes/ssl/
        ExecStartPre=/bin/bash kubeconfig.sh 
        ExecStart=/bin/bash kubeconfig.sh main
        ExecReload=/bin/bash kubeconfig.sh main
        KillMode=process
        RemainAfterExit=yes

    - name: flanneld.service
      command: start
      drop-ins:
      - name: 50-network-config.conf
        content: |
          [Service]
          ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{"Network":"10.0.1.0/16", "Backend": {"Type": "vxlan"}}'

    - name: etcd2.service
      command: start
      enable: true

    - name: fleet.service
      command: start
      enable: true

  etcd2:
     name: master1
     initial-advertise-peer-urls: http://10.1.0.20:2380
     listen-peer-urls: http://10.1.0.20:2380
     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
     advertise-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
     initial-cluster-token: cluster-ellesmera
     initial-cluster: master1=http://10.1.0.20:2380
     initial-cluster-state: new
