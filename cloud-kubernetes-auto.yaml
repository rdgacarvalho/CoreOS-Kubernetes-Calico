#cloud-config
hostname: "kubemaster"

ssh_authorized_keys:
  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD037qxy/sqLljZ/50+jFFTArgx0SYRytQcRIbIvUusdLG5CK/oHF+ZfSNZLNQUFuKyaUOJWXaug7VOzqIGliaNvXsoB5iKTP5RXdFXdZ2oCzPx/bXbCdxqfvV78GVIb0o/r4xI8xlR326gwDhDpb/Oy0R2PP98VBLhIWc1T5C4zGESd/IuPmWHEYl4c4ctG1qVgaOIz52RawM22Q/4jXpAbitFiKMJxyz6olQy61zZ9e77u9Ad4c2cHW8gTGiz8eYCR5dTHV38jFk7NxwHzYWdHQ0XmbVRXgzHuLQjCwfkpuqDvZujoam8deDwcdbESgqalugLKHANv8+bjiqnc1EZ root@glaedr.ellesmera
  - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCWM7xMnzcy1hTjxoV7/Cc8lSeoWhUUPsoNbFSkL9wiLq++SqscipEeA3HRYlI7X9VJbhCWRYAA5AKj7hGMZ60sByzvo8nSJVN54zcOfH5wnJKdO3FJeQfYkyqYsBpC8Kg0+dILtvhxgf5KenFQkPnb8DXmNExvm/jzJgu2YbOSmKcSvt9zQnHTj7AAILC8GozYonJEzE5Yo8JiKUbVSBq9lL54dbi/47QhfhIjKbK7n4+tbsU/aLPKauv+NW2jj69tIycg6WnoarSeUNZy8/zUtcMqwfrilxuDw+9Gl+o+aXeVbwpkOw6D0bPeWkOwY0CkdyOKfwiW5tP4VUF70VOr drigocar@AC049707

users:
  - name: "rodrigo"
    passwd: "$1$GFuzo/U6$aF7g4o.S2OE9iv9v/On61."
    groups:
      - "sudo"
      - "docker"
    ssh-authorized-keys:
    - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD037qxy/sqLljZ/50+jFFTArgx0SYRytQcRIbIvUusdLG5CK/oHF+ZfSNZLNQUFuKyaUOJWXaug7VOzqIGliaNvXsoB5iKTP5RXdFXdZ2oCzPx/bXbCdxqfvV78GVIb0o/r4xI8xlR326gwDhDpb/Oy0R2PP98VBLhIWc1T5C4zGESd/IuPmWHEYl4c4ctG1qVgaOIz52RawM22Q/4jXpAbitFiKMJxyz6olQy61zZ9e77u9Ad4c2cHW8gTGiz8eYCR5dTHV38jFk7NxwHzYWdHQ0XmbVRXgzHuLQjCwfkpuqDvZujoam8deDwcdbESgqalugLKHANv8+bjiqnc1EZ root@glaedr.ellesmera
    - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCWM7xMnzcy1hTjxoV7/Cc8lSeoWhUUPsoNbFSkL9wiLq++SqscipEeA3HRYlI7X9VJbhCWRYAA5AKj7hGMZ60sByzvo8nSJVN54zcOfH5wnJKdO3FJeQfYkyqYsBpC8Kg0+dILtvhxgf5KenFQkPnb8DXmNExvm/jzJgu2YbOSmKcSvt9zQnHTj7AAILC8GozYonJEzE5Yo8JiKUbVSBq9lL54dbi/47QhfhIjKbK7n4+tbsU/aLPKauv+NW2jj69tIycg6WnoarSeUNZy8/zUtcMqwfrilxuDw+9Gl+o+aXeVbwpkOw6D0bPeWkOwY0CkdyOKfwiW5tP4VUF70VOr drigocar@AC049707

write_files:
    - path: /etc/hosts
      owner: root
      content: |
             192.168.1.5    node1 node1.ellesmera.intranet
             10.1.0.5       worker1  worker1 master.ellesmera.intranet
             192.168.1.6    node2 node2.ellesmera.intranet
             10.1.0.6       worker2 worker2.ellesmera.intranet
             192.168.1.7    node3 node3.ellesmera.intranet
             10.1.0.7       worker3 worker3.ellesmera.intranet
             192.168.1.20   kubemaster kubemaster.ellesmera.intranet
             10.1.0.20      master1 master1.ellesmera.intranet

    - path: /etc/resolv.conf
      content: |
        search ellesmera
        nameserver 201.17.1.97
        nameserver 192.168.1.1

    - path: /opt/bin/create_dirs
      owner: root
      permissions: 0755
      content: |
          #!/usr/bin/bash
          mkdir -p /etc/kubernetes/
          mkdir -p /etc/kubernetes/ssl/
          mkdir -p /etc/kubernetes/manifests/
          mkdir -p /etc/flannel/
          mkdir -p /etc/systemd/system/flanneld.service.d/
          mkdir -p /etc/systemd/system/docker.service.d/
          mkdir -p /etc/kubernetes/cni/net.d/

    # Configure ca-certificates
    - path: /etc/kubernetes/ssl/openssl.cnf
      owner: root
      content: |
          [req]
          req_extensions = v3_req
          distinguished_name = req_distinguished_name
          [req_distinguished_name]
          [ v3_req ]
          basicConstraints = CA:FALSE
          keyUsage = nonRepudiation, digitalSignature, keyEncipherment
          subjectAltName = @alt_names
          [alt_names]
          DNS.1 = kubernetes
          DNS.2 = kubernetes.default
          DNS.3 = kubernetes.default.svc
          DNS.4 = kubernetes.default.svc.cluster.local
          IP.1 = ${K8S_SERVICE_IP}
          IP.2 = ${MASTER_HOST}

    # Kubernetes Worker Keypairs
    - path: /etc/kubernetes/ssl/worker-openssl.cnf
      owner: root
      content: |
          [req]
          req_extensions = v3_req
          distinguished_name = req_distinguished_name
          [req_distinguished_name]
          [ v3_req ]
          basicConstraints = CA:FALSE
          keyUsage = nonRepudiation, digitalSignature, keyEncipherment
          subjectAltName = @alt_names
          [alt_names]
          IP.1 = $ENV::WORKER_IP

    # Kubernetes Script Install
    - path: /opt/bin/kubernetes-install.sh
      owner: root
      permissions: 0755
      content: |
        #! /usr/bin/bash
        if [ ! -f /opt/bin/kubelet ]; then
          echo "Kubenetes not installed - installing."
          # Extract the Kubernetes binaries.
          wget -N -P /opt/bin http://storage.googleapis.com/kubernetes-release/release/v1.3.4/bin/linux/amd64/kubectl
          wget -N -P /opt/bin http://storage.googleapis.com/kubernetes-release/release/v1.3.4/bin/linux/amd64/kubelet
          sudo chmod +x /opt/bin/kubelet /opt/bin/kubectl
        fi

    # Calico Script Install
    - path: /opt/bin/calico-install.sh
      owner: root
      permissions: 0755
      content: |
        #! /usr/bin/bash
        if [ ! -f /opt/bin/calicoctl ]; then
          echo "Calico not installed - installing."
          # Install the `calicoctl` binary
          wget https://github.com/projectcalico/calico-containers/releases/download/v0.14.0/calicoctl
          chmod +x calicoctl
          mv calicoctl /opt/bin
          # Fetch the calico/node container
          sudo docker pull calico/node:v0.14.0
        fi

    #Kubernetes Script Config
    - path: /etc/kubernetes/ssl/kubeconfig.sh
      owner: root
      permissions: 0755
      content: |
        #!/usr/bin/bash

        #set -x

        ###################################
        # Script configuracao Kubernetes
        ###################################

        function create_ca()
        {

          echo "Create a Cluster Root CA ...";

          if [ ! -f /etc/kubernetes/ssl/ca-key.pem ]; then
            openssl genrsa -out ca-key.pem 2048
            openssl req -x509 -new -nodes -key ca-key.pem -days 10000 -out ca.pem -subj "/CN=kube-ca"
          fi

          config_cluster

        }

        function config_cluster()
        {

          echo "Set up IPs ...";

          HOST_IP=192.168.1.20
          K8S_IP=10.3.0.1

          eval sed -i -e "s#\\\${MASTER_HOST}#"$HOST_IP"#g" openssl.cnf
          eval sed -i -e "s#\\\${K8S_SERVICE_IP}#"$K8S_IP"#g" openssl.cnf
          echo "Done.";

          create_keypair

        }


        function create_keypair()
        {

          echo "Generate the API Server Keypair ...";

          if [ ! -f apiserver-key.pem -a ! -f apiserver.csr ]; then
           openssl genrsa -out apiserver-key.pem 2048
           openssl req -new -key apiserver-key.pem -out apiserver.csr -subj "/CN=kube-apiserver" -config openssl.cnf
           openssl x509 -req -in apiserver.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out apiserver.pem -days 365 -extensions v3_req -extfile openssl.cnf
          fi
          echo "Done ...";

          create_worker

        }

        function create_worker()
        {

          echo "Generate the Kubernetes Worker Keypairs ...";

          WORKER_NUMBER=1
          WORKERS_FQDN=("worker1" "worker2" "worker3")
          WORKERS_IP=("192.168.1.5" "192.168.1.6" "192.168.1.7")
          MASTER_IP=("192.168.1.20")
          K8S_SERVICE_IP=10.3.0.1

          if [ ! -f /etc/kubernetes/ssl/worker*.pem -a ! -f /etc/kubernetes/ssl/worker*.csr ]; then
            echo "Generating TLS keys."
            openssl genrsa -out worker-key.pem 2048
            WORKER_IP="${MASTER_IP}" openssl req -new -key worker-key.pem -out worker.csr -subj "/CN=worker" -config worker-openssl.cnf
            WORKER_IP="${MASTER_IP}" openssl x509 -req -in worker.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf

            for ((i=0; i <= "$WORKER_NUMBER"; i++));
            do
              echo "${WORKERS_IP[i]}" --- "${WORKERS_FQDN[i]}";
              openssl genrsa -out "${WORKERS_FQDN[i]}"-worker-key.pem 2048
              WORKER_IP="${WORKERS_IP[i]}" openssl req -new -key "${WORKERS_FQDN[i]}"-worker-key.pem -out "${WORKERS_FQDN[i]}"-worker.csr -subj "/CN="${WORKERS_FQDN[i]}"" -config worker-openssl.cnf
              WORKER_IP="${WORKERS_IP[i]}" openssl x509 -req -in "${WORKERS_FQDN[i]}"-worker.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out "${WORKERS_FQDN[i]}"-worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf
              sleep 2;
              tar cfv "${WORKERS_FQDN[i]}".tgz ca.pem "${WORKERS_FQDN[i]}"-worker-key.pem "${WORKERS_FQDN[i]}"-worker.pem
              tar tf "${WORKERS_FQDN[i]}".tgz 
              scp "${WORKERS_FQDN[i]}".tgz core@"${WORKERS_FQDN[i]}":~/
            done
          fi

          create_adminkey

        }

        function create_adminkey()
        {

          # Generate the Cluster Administrator Keypair
          
          if [ ! -f admin-key.pem -a ! -f admin.csr ]; then
           openssl genrsa -out admin-key.pem
           openssl req -new -key admin-key.pem -out admin.csr -subj "/CN=kube-admin"
           openssl x509 -req -in admin.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out admin.pem -days 365
          fi

          create_dirs

        }

        function create_dirs()
        {

          # Automated creatre directories
          sh create_dirs

          kubernetes_setup

        }

        function kubernetes_setup()
        {

          ADV_IP=192.168.1.20
          ETCD_END=http://10.1.0.5:2379,http://10.1.0.6:2379,http://10.1.0.20:2379
          K8S=v1.4.6_coreos.0
          NETWORK=cni
          SERVICE_RANGE=10.3.0.0/24
          DNS_SERVICE=10.3.0.10

          echo "Flannel Network Configuration ..."
          eval sed -i -e 's#\\\${ADVERTISE_IP}#"$ADV_IP"#g' /etc/flannel/options.env
          eval sed -i -e 's#\\\${ETCD_ENDPOINTS}#"$ETCD_END"#g' /etc/flannel/options.env

          echo "Create the kubelet Unit ..."
          eval sed -i -e 's#\\\${ADVERTISE_IP}#"$ADV_IP"#g' /etc/systemd/system/kubelet.service
          eval sed -i -e 's#\\\${DNS_SERVICE_IP}#"$DNS_SERVICE"#g' /etc/systemd/system/kubelet.service
          eval sed -i -e 's#\\\${K8S_VER}#"$K8S"#g' /etc/systemd/system/kubelet.service
          eval sed -i -e 's#\\\${NETWORK_PLUGIN}#"$NETWORK"#g' /etc/systemd/system/kubelet.service

          echo "Set Up the kube-apiserver Pod ..."
          eval sed -i -e 's#\\\${ETCD_ENDPOINTS}#"$ETCD_END"#g' /etc/kubernetes/manifests/kube-apiserver.yaml
          eval sed -i -e 's#\\\${SERVICE_IP_RANGE}#"$SERVICE_RANGE"#g' /etc/kubernetes/manifests/kube-apiserver.yaml
          eval sed -i -e 's#\\\${ADVERTISE_IP}#"$ADV_IP"#g' /etc/kubernetes/manifests/kube-apiserver.yaml

          echo "Set Up Calico Node Container (optional) ..."
          eval sed -i -e 's#\\\${ADVERTISE_IP}#"$ADV_IP"#g' /etc/systemd/system/calico-node.service
          eval sed -i -e 's#\\\${ETCD_ENDPOINTS}#"$ETCD_END"#g' /etc/systemd/system/calico-node.service

          echo "Set Up the policy-controller Pod (optional) ..."
          eval sed -i -e 's#\\\${ETCD_ENDPOINTS}#"$ETCD_END"#g' /etc/kubernetes/manifests/policy-controller.yaml

          echo "Set Up the CNI config (optional) ..."
          eval sed -i -e 's#\\\${ADVERTISE_IP}#"$ADV_IP"#g' /etc/kubernetes/cni/net.d/10-calico.conf
          eval sed -i -e 's#\\\$ETCD_ENDPOINTS#"$ETCD_END"#g' /etc/kubernetes/cni/net.d/10-calico.conf

          setup_certs

        }

        function setup_certs()
        {

          echo "TLS Assets ...";
          chmod 600 /etc/kubernetes/ssl/*-key.pem
          chown root:root /etc/kubernetes/ssl/*-key.pem

          setup_kubectl

        }

        function setup_kubectl()
        {

          CA_CERT=/etc/kubernetes/ssl/ca.pem
          ADMIN_KEY=/etc/kubernetes/ssl/admin-key.pem
          ADMIN_CERT=/etc/kubernetes/ssl/admin.pem
          MASTER_HOST=192.168.1.20

          sh /opt/bin/kubernetes-install.sh
          sh /opt/bin/calico-install.sh

          /opt/bin/kubectl config set-cluster default-cluster --server=https://"${MASTER_HOST}" --certificate-authority="${CA_CERT}"
          /opt/bin/kubectl config set-credentials default-admin --certificate-authority="${CA_CERT}" --client-key="${ADMIN_KEY}" --client-certificate="${ADMIN_CERT}"
          /opt/bin/kubectl config set-context default-system --cluster=default-cluster --user=default-admin
          /opt/bin/kubectl config use-context default-system

          #curl -H "Content-Type: application/json" -XPOST -d'{"apiVersion":"v1","kind":"Namespace","metadata":{"name":"kube-system"}}' "http://127.0.0.1:8080/api/v1/namespaces"
          #curl -H "Content-Type: application/json" -XPOST -d'{"apiVersion":"v1","kind":"Namespace","metadata":{"name":"calico-system"}}' "http://127.0.0.1:8080/api/v1/namespaces"

          daemon_reload

        }

        function daemon_reload()
        {

          echo "Saving settings ..."
          systemctl daemon-reload
          systemctl restart etcd2

        }

        ############### Start Script ####################

        echo "Kubernetes set up start...";
        echo "core:05digo10" | chpasswd core
        echo "05digo10" | ssh-copy-id -i ~core/.ssh/id_rsa.pub core@worker1
        echo "05digo10" | ssh-copy-id -i ~core/.ssh/id_rsa.pub core@worker2
        echo "";
        create_ca

    # Network Configuration
    - path: /etc/flannel/options.env
      owner: root
      content: |
          FLANNELD_IFACE=${ADVERTISE_IP}
          FLANNELD_ETCD_ENDPOINTS=${ETCD_ENDPOINTS}
    # Network Configuration
    - path: /etc/systemd/system/flanneld.service.d/40-ExecStartPre-symlink.conf
      content: |
          [Service]
          ExecStartPre=/usr/bin/ln -sf /etc/flannel/options.env /run/flannel/options.env
    # Docker Configuration
    - path: /etc/systemd/system/docker.service.d/40-flannel.conf
      owner: root
      content: |
        [Unit]
        Requires=flanneld.service
        After=flanneld.service
    # Create the kubelet Unit
    - path: /etc/systemd/system/kubelet.service
      content: |
        [Service]
        ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
        Environment=KUBELET_VERSION=${K8S_VER}
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
          --api-servers=http://127.0.0.1:8080 \
          --network-plugin-dir=/etc/kubernetes/cni/net.d \
          --network-plugin=${NETWORK_PLUGIN} \
          --register-schedulable=false \
          --allow-privileged=true \
          --config=/etc/kubernetes/manifests \
          --hostname-override=${ADVERTISE_IP} \
          --cluster-dns=${DNS_SERVICE_IP} \
          --cluster-domain=cluster.local
        Restart=always
        RestartSec=10
        [Install]
        WantedBy=multi-user.target
    # Set Up the kube-apiserver Pod
    - path: /etc/kubernetes/manifests/kube-apiserver.yaml
      owner: root
      content: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-apiserver
          namespace: kube-system
        spec:
          hostNetwork: true
          containers:
          - name: kube-apiserver
            image: quay.io/coreos/hyperkube:v1.4.6_coreos.0
            command:
            - /hyperkube
            - apiserver
            - --bind-address=0.0.0.0
            - --etcd-servers=${ETCD_ENDPOINTS}
            - --allow-privileged=true
            - --service-cluster-ip-range=${SERVICE_IP_RANGE}
            - --secure-port=443
            - --advertise-address=${ADVERTISE_IP}
            - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota
            - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
            - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
            - --client-ca-file=/etc/kubernetes/ssl/ca.pem
            - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
            - --runtime-config=extensions/v1beta1=true,extensions/v1beta1/networkpolicies=true
            ports:
            - containerPort: 443
              hostPort: 443
              name: https
            - containerPort: 8080
              hostPort: 8080
              name: local
            volumeMounts:
            - mountPath: /etc/kubernetes/ssl
              name: ssl-certs-kubernetes
              readOnly: true
            - mountPath: /etc/ssl/certs
              name: ssl-certs-host
              readOnly: true
          volumes:
          - hostPath:
              path: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
          - hostPath:
              path: /usr/share/ca-certificates
            name: ssl-certs-host
    # Set Up the kube-proxy Pod
    - path: /etc/kubernetes/manifests/kube-proxy.yaml
      owner: root
      content: |
          apiVersion: v1
          kind: Pod
          metadata:
            name: kube-proxy
            namespace: kube-system
          spec:
            hostNetwork: true
            containers:
            - name: kube-proxy
              image: quay.io/coreos/hyperkube:v1.4.6_coreos.0
              command:
              - /hyperkube
              - proxy
              - --master=http://127.0.0.1:8080
              - --proxy-mode=iptables
              securityContext:
                privileged: true
              volumeMounts:
              - mountPath: /etc/ssl/certs
                name: ssl-certs-host
                readOnly: true
            volumes:
            - hostPath:
                path: /usr/share/ca-certificates
              name: ssl-certs-host
    # Set Up the kube-controller-manager Pod
    - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
      owner: root
      content: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-controller-manager
          namespace: kube-system
        spec:
          hostNetwork: true
          containers:
          - name: kube-controller-manager
            image: quay.io/coreos/hyperkube:v1.4.6_coreos.0
            command:
            - /hyperkube
            - controller-manager
            - --master=http://127.0.0.1:8080
            - --leader-elect=true
            - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
            - --root-ca-file=/etc/kubernetes/ssl/ca.pem
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                path: /healthz
                port: 10252
              initialDelaySeconds: 15
              timeoutSeconds: 1
            volumeMounts:
            - mountPath: /etc/kubernetes/ssl
              name: ssl-certs-kubernetes
              readOnly: true
            - mountPath: /etc/ssl/certs
              name: ssl-certs-host
              readOnly: true
          volumes:
          - hostPath:
              path: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
          - hostPath:
              path: /usr/share/ca-certificates
            name: ssl-certs-host
    # Set Up the kube-scheduler Pod
    - path: /etc/kubernetes/manifests/kube-scheduler.yaml
      owner: root
      content: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-scheduler
          namespace: kube-system
        spec:
          hostNetwork: true
          containers:
          - name: kube-scheduler
            image: quay.io/coreos/hyperkube:v1.4.6_coreos.0
            command:
            - /hyperkube
            - scheduler
            - --master=http://127.0.0.1:8080
            - --leader-elect=true
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                path: /healthz
                port: 10251
              initialDelaySeconds: 15
              timeoutSeconds: 1
    # Set Up the kube-apiserver Pod
    - path: /etc/kubernetes/worker-kubeconfig.yaml
      owner: root
      content: |
          apiVersion: v1
          kind: Config
          clusters:
          - name: local
            cluster:
              certificate-authority: /etc/kubernetes/ssl/ca.pem
          users:
          - name: kubelet
            user:
              client-certificate: /etc/kubernetes/ssl/worker.pem
              client-key: /etc/kubernetes/ssl/worker-key.pem
          contexts:
          - context:
              cluster: local
              user: kubelet
            name: kubelet-context
          current-context: kubelet-context
    # Set Up Calico Node Container (optional)
    - path: /etc/systemd/system/calico-node.service
      owner: root
      content: |
        [Unit]
        Description=Calico per-host agent
        Requires=network-online.target
        After=network-online.target
        [Service]
        Slice=machine.slice
        Environment=CALICO_DISABLE_FILE_LOGGING=true
        Environment=HOSTNAME=${ADVERTISE_IP}
        Environment=IP=${ADVERTISE_IP}
        Environment=FELIX_FELIXHOSTNAME=${ADVERTISE_IP}
        Environment=CALICO_NETWORKING=false
        Environment=NO_DEFAULT_POOLS=true
        Environment=ETCD_ENDPOINTS=${ETCD_ENDPOINTS}
        ExecStart=/usr/bin/rkt run --inherit-env --stage1-from-dir=stage1-fly.aci \
        --volume=modules,kind=host,source=/lib/modules,readOnly=false \
        --mount=volume=modules,target=/lib/modules \
        --trust-keys-from-https quay.io/calico/node:v0.19.0
        KillMode=mixed
        Restart=always
        TimeoutStartSec=0
        [Install]
        WantedBy=multi-user.target
    # Set Up the policy-controller Pod (optional)
    - path: /etc/kubernetes/manifests/policy-controller.yaml
      owner: root
      content: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: calico-policy-controller
          namespace: calico-system
        spec:
          hostNetwork: true
          containers:
            # The Calico policy controller.
            - name: k8s-policy-controller
              image: calico/kube-policy-controller:v0.2.0
              env:
                - name: ETCD_ENDPOINTS
                  value: "${ETCD_ENDPOINTS}"
                - name: K8S_API
                  value: "http://127.0.0.1:8080"
                - name: LEADER_ELECTION
                  value: "true"
            # Leader election container used by the policy controller.
            - name: leader-elector
              image: quay.io/calico/leader-elector:v0.1.0
              imagePullPolicy: IfNotPresent
              args:
                - "--election=calico-policy-election"
                - "--election-namespace=calico-system"
                - "--http=127.0.0.1:4040"
    # Set Up the CNI config (optional)
    - path: /etc/kubernetes/cni/net.d/10-calico.conf
      owner: root
      content: |
        {
            "name": "calico",
            "type": "flannel",
            "delegate": {
                "type": "calico",
                "etcd_endpoints": "$ETCD_ENDPOINTS",
                "log_level": "none",
                "log_level_stderr": "info",
                "hostname": "${ADVERTISE_IP}",
                "policy": {
                    "type": "k8s",
                    "k8s_api_root": "http://127.0.0.1:8080/api/v1/"
                }
            }
        }
coreos:
  units:

    - name: static.network
      content: |
        [Match]
        Name=enp0s3
        [Network]
        Address=10.1.0.20/24
        Gateway=10.1.0.1

    - name: 11-static.network
      content: |
        [Match]
        Name=enp0s8
        [Network]
        Address=10.2.0.20/24

    - name: 13-static.network
      content: |
        [Match]
        Name=enp0s9
        [Network]
        Address=192.168.1.20/24

    - name: cpu-governor.service
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Enable CPU power saving
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStart=/usr/sbin/modprobe cpufreq_powersave

    - name: kubeconfig.service
      command: start
      enable: true
      content: |
        [Unit]
        Description=Enable Kubeconfig Service
        After=systemd-networkd.service
        Requires=systemd-networkd.service

        [Service]
        Type=oneshot
        ExecStart=/etc/kubernetes/ssl/kubeconfig.sh
        ExecReload=/etc/kubernetes/ssl/kubeconfig.sh
        RemainAfterExit=yes

    - name: systemd-networkd.service
      command: start
      enable: true

    - name: etcd2.service
      command: start
      enable: true

    - name: kubelet.service
      command: start
      enable: true

    - name: calico-node.service
      command: start
      enable: true

    - name: fleet.service
      command: start
      enable: true

    - name: flanneld.service
      command: start
      drop-ins:
      - name: 50-network-config.conf
        content: |
          [Service]
          ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{"Network":"10.0.1.0/16", "Backend": {"Type": "vxlan"}}'
  etcd2:
     name: master1
     initial-advertise-peer-urls: http://10.1.0.20:2380
     listen-peer-urls: http://10.1.0.20:2380
     listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
     advertise-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
     initial-cluster-token: cluster-ellesmera
     initial-cluster: worker1=http://10.1.0.5:2380,worker2=http://10.1.0.6:2380,master1=http://10.1.0.20:2380
     initial-cluster-state: new
